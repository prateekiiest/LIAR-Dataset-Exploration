# -*- coding: utf-8 -*-
"""LIAR-Data Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uv517LPMJEvBnQppcvxJqkt9o7QhwiXy

# Analyzing LIAR Plus Data Set

## DataSet Visualization

### Loading Training Data Set
"""

import pandas as pd 
# Read data from file 'filename.csv' 
# (in the same directory that your python process is based)
# Control delimiters, rows, column names with read_csv (see later) 
data = pd.read_csv("LIAR-PLUS-master/dataset/train2.tsv",sep='\t',
                   names=["Statement ID", "Label", "Statement", "Subject", "Speaker", "Speaker Job's title", "State info", 
                          "Party Affiliate", "barely true counts", "false counts", "half true counts", "mostly true counts", "pants on fire counts", "venue", "Extracted Justification"]) 
# Preview the first 5 lines of the loaded data 
data.head()

"""### DataSet Available at [Tariq/LIAR-Plus](https://github.com/Tariq60/LIAR-PLUS)


* Column 1: the ID of the statement ([ID].json).
* Column 2: the label.
* Column 3: the statement.
* Column 4: the subject(s).
* Column 5: the speaker.
* Column 6: the speaker's job title.
* Column 7: the state info.
* Column 8: the party affiliation.
* Columns 9-13: the total credit history count, including the current statement.
    * 9: barely true counts.
    * 10: false counts.
    * 11: half true counts.
    * 12: mostly true counts.
    * 13: pants on fire counts.
* Column 14: the context (venue / location of the speech or statement).
* Column 15: the extracted justification
"""

print(data.dtypes)
print(data.shape)

import pandas as pd 
# Read data from file 'filename.csv' 
# (in the same directory that your python process is based)
# Control delimiters, rows, column names with read_csv (see later) 
test_data = pd.read_csv("LIAR-PLUS-master/dataset/train2.tsv",sep='\t',
                   names=["Statement ID", "Label", "Statement", "Subject", "Speaker", "Speaker Job's title", "State info", 
                          "Party Affiliate", "barely true counts", "false counts", "half true counts", "mostly true counts", "pants on fire counts", "venue", "Extracted Justification"]) 
# Preview the first 5 lines of the loaded data 
test_data.head()

data["Label"].head(20)

"""### Binary Classification

As we see from the data its a multi-class data having 6 classes namely "false", ""mostly-true", "barely-true", "half-true", "true" , "pants-fire"

For the purpose of Binary Classification, we simply consider labels  ""mostly-true", "barely-true", "half-true", "true" as "true" and "false" "pants-fire" as false

#### Converting Training Data to Binary Labelled Data
"""

bin_data = data

bin_data['Label'] = bin_data['Label'].replace(['false', 'pants-fire'], 0)

bin_data['Label'] = bin_data['Label'].replace(['mostly-true', 'half-true', 'barely-true', 'true'], 1)

bin_data['Label'].head(10)

"""#### Converting Test Data to Binary Labelled Data"""

test_bin_data = test_data

test_bin_data['Label'] = test_bin_data['Label'].replace(['false', 'pants-fire'], 0)
test_bin_data['Label'] = test_bin_data['Label'].replace(['mostly-true', 'half-true', 'barely-true', 'true'], 1)

def evaluate(model, test_set, model_name):
    
    y_pred = model.predict(test_set['Subject'].values.astype('U'))
    y_true = test_set['Label']
    f1 = f1_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    accuracy = accuracy_score(y_true, y_pred)
    
    print('::::: Evaluation Results :::: {}'.format(model_name))
    print('Accuracy is: {}'.format(accuracy))
    print('F1 score is: {}'.format(f1))
    print('Precision score is: {}'.format(precision))
    print('Recall score is: {}'.format(recall))
    return accuracy

import pandas as pd
from nltk.corpus import stopwords
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, f1_score, precision_score,
                             recall_score)

from sklearn.pipeline import Pipeline

"""### Linear Regression"""

lr_pipeline = Pipeline([
    ('lrCV', CountVectorizer(stop_words="english", lowercase=True, ngram_range=(1, 1))),
    ('lr_clf', LogisticRegression(C=0.0001,random_state=42, n_jobs=-1))
])

lr_pipeline.fit(bin_data['Subject'].values.astype('U'), bin_data['Label'])

lr_acc = evaluate(lr_pipeline, test_bin_data, 'Logistic Regression')

"""### Support Vector Machine"""

from sklearn.feature_extraction import text
from sklearn.svm import SVC

svm_pipeline = Pipeline([
    ('svm_CV', CountVectorizer(stop_words="english", lowercase=False, ngram_range=(1, 1))),
    ('svm_clf', SVC(random_state=42, gamma=1.0, kernel='rbf'))
])

svm_pipeline.fit(bin_data['Subject'].values.astype('U'), bin_data['Label'])

svm_acc = evaluate(svm_pipeline, test_bin_data, 'SVM Count Vectorizer')

"""### Naive Bayes Classification"""

from sklearn.naive_bayes import MultinomialNB

nb_pipeline = Pipeline([
    ('nb_CV', CountVectorizer(stop_words="english", lowercase=True, ngram_range=(1, 10))),
    ('nb_clf', MultinomialNB(alpha=6.8))
])

nb_pipeline.fit(bin_data['Subject'].values.astype('U'), bin_data['Label'])

nb_acc = evaluate(nb_pipeline, test_bin_data, 'Naive Bayes Count Vectorizer')

"""### Random Forest Classifier"""

rf_pipeline = Pipeline([
    ('rf_CV', CountVectorizer(stop_words="english", lowercase=False, ngram_range=(1, 1))),
    ('rf_clf', RandomForestClassifier(max_depth=12, n_estimators=300, n_jobs=-1, random_state=42))
])

rf_pipeline.fit(bin_data['Subject'].values.astype('U'), bin_data['Label'])

rf_acc = evaluate(rf_pipeline, test_data, 'Random Forest')

"""## Multi Classification"""

def accuracy(model, test_set, model_name):
    
    y_pred = model.predict(test_set['Subject'].values.astype('U'))
    y_true = test_set['Label']
    #f1 = f1_score(y_true, y_pred)
    #precision = precision_score(y_true, y_pred)
    #recall = recall_score(y_true, y_pred)
    accuracy = accuracy_score(y_true, y_pred)
    
    print('::::: Evaluation Results :::: {}'.format(model_name))
    print('Accuracy is: {}'.format(accuracy))
    return accuracy

"""### Linear Regression"""

lr_pipeline.fit(data['Subject'].values.astype('U'), data['Label'])

lr_mc_acc = accuracy(lr_pipeline, test_data, 'Logistic Regression ')

"""### Support Vector Machine"""

svm_pipeline.fit(data['Subject'].values.astype('U'), data['Label'])

svm_mc_acc = accuracy(svm_pipeline, test_data, 'SVM Count Vectorizer')

"""### Naive Bayes"""

nb_pipeline.fit(data['Subject'].values.astype('U'), data['Label'])

nb_mc_acc = accuracy(nb_pipeline, test_data, 'Naive Bayes')

"""## Classification Results"""

import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

x_bin = ["LR", "SVM", "NB"]
y_bin = [lr_acc,svm_acc,nb_acc]

sns.barplot(x_bin, y_bin, palette="Blues")
plt.title("Binary Classification Accuracy")

x_mc = ["LR", "SVM", "NB"]
y_mc = [lr_mc_acc,svm_mc_acc,nb_mc_acc]

sns.barplot(x_mc, y_mc, palette="Blues")
plt.title("Multi Classification Accuracy")

"""As we see binary classification gives better results since we only want to label a statement as either true or false.

In case of six way classification, it gives worse results, due to the fact we have more restrictions in classifying within the 
6 categories. Hence we need to analyse specific features to build a stronger classifier.

---------------------------------------

## Analyzing Feature Set from the Data

Some of the features that define the data set include the Subject of the Statement, the Speaker of the statement, the speaker's job and the party affiliation.

### Exploring the Subject / Topic Feature

The feature subject may be interesting in determining the truthfulness of a statement.

First, we see how many unique subjects or topics are present in the data and how the truthfulness of the statement depends on 
the topic of a statement.
"""

data.columns

def topic_data(df):
    
    df = df.copy()
    df["Subject"] = df["Subject"].apply(lambda x : str(x).lower().split(","))
    ## Create a dataframe of all subjects
    subjects = df.Subject.apply(pd.Series)
    cols = list(df.columns.values)
    cols.remove("Subject")
   
    df = subjects.merge(df, right_index = True, left_index = True) \
        .drop(["Subject"], axis = 1)
        
       
    lf = pd.melt(df, id_vars = cols, value_name = "Subject") \
        .drop("variable", axis = 1) \
    
    return lf

df_raw = data.sample(frac=1).reset_index()
topic_data = topic_data(df_raw)

set_of_subjects = set(topic_data['Subject'])
    
print("Total %d unique subjects" % len(set_of_subjects))
print("Sample subjects:\n", list(set_of_subjects)[:10])

"""Finding unique subjects occuring more than 200 times"""

subject_counts = topic_data.groupby("Subject").count()
subjects_200 = subject_counts.where(subject_counts['Statement ID'] > 200).dropna().index
topic_data = topic_data[topic_data['Subject'].isin(subjects_200)]

topic_data.head(5)

count_df = topic_data.groupby(["Subject","Label"]).agg({"Statement ID" : "count"})
count_df.unstack().plot(kind='bar', stacked=True, figsize=(20,5))

"""Here, we find a distribution showing the relation between the truthfulness of a statement with its corresponding topic

### Exploring the Speaker Feature

The feature speaker may also contribute significantly in determining the truthfulness of a statement.

First, we see how many unique speakers are there in the data.

Next we wish to see which speakers are more likely to make a false claim and which speakers are most truthful.
"""

speakers_df = df_raw.copy()
speakers = speakers_df['Speaker'].unique()
print("Total %d unique speakers" % len(speakers))
print("Sample speakers:\n", speakers[:10])

speakers_cts = speakers_df.groupby("Speaker").Statement.count()
speakers = speakers_cts[speakers_cts > 20]
print("Total %d unique speakers who appear more than 20 times within the dataset" % len(speakers))
print("Sample speakers:\n", speakers[:10])

speakers_df = speakers_df[speakers_df['Speaker'].isin(speakers.keys())]

speakers_df.head(5)

count_speakers = speakers_df.groupby(["Speaker","Label"]).agg({"Statement ID" : "count"})
count_speakers.unstack().plot(kind='bar', stacked=True, figsize=(20,5))

"""For example, an interesting feature from the distribution is that **Donal Trump** is always making *false claims* most of the time. [ "true" label is very less]

### Exploring the Job Feature

The feature **Job** may also contribute significantly in determining the truthfulness of a statement.

Some speakers belonging to particular job may make more false claims while others may make more truthful claims
"""

jobs_df = data.copy()
jobs = jobs_df['Speaker Job\'s title'].unique()
print("Total %d unique jobs" % len(jobs))
print("Sample jobs:\n", jobs[:10])

job_cts = jobs_df.groupby("Speaker Job\'s title").Statement.count()
jobs = job_cts[job_cts > 20]
print("There are %d unique jobs who appear more than 20 times within the dataset" % len(jobs))
print("Some sample jobs include:\n", jobs[:5])

jobs

jobs_df = jobs_df[jobs_df['Speaker Job\'s title'].isin(jobs.keys())]

jobs_count = jobs_df.groupby(["Speaker Job\'s title","Label"]).agg({"Statement ID" : "count"})
jobs_count.unstack().plot(kind='bar', stacked=True, figsize=(20,5))

"""Interestingly **President-Elect** or president to be elected has the higher number of cases where he/she makes a false claim.

### Exploring the Party Affiliation Feature

Members of a particular party may be more prone to making false claims than others.
Hence an important step is to study such a relation between party affiliation and the truthfulness of their claims.
"""

parties_df = df_raw.copy()
parties = jobs_df['Party Affiliate'].unique()
print("Total %d unique parties" % len(parties))
print("Sample parties:\n", parties)

parties_cts = parties_df.groupby("Party Affiliate").Statement.count()
parties = parties_cts[parties_cts > 30]
print("Total %d unique affiliations who appear more than 30 times" % len(parties))
print("Sample affiliations:\n", parties[:5])

parties_df = parties_df[parties_df['Party Affiliate'].isin(parties.keys())]

parties_df.head()

parties_count = parties_df.groupby(["Party Affiliate","Label"]).agg({"Statement ID" : "count"})
parties_count.unstack().plot(kind='bar', stacked=True, figsize=(20,5))

"""## Sentiment Analysis

An important analysis in this paper states that by considering the  sentiment of the statement in question. The paper considers Sentistrength
library for their purpose.

By ranking the sentiment of the statement from a range of 0 to 5 indicating negative to neutral and to positive, it may give some interesting
insights as to whether a statement is truthful or not
"""

import nltk

import vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()

sentiments = pd.DataFrame([analyzer.polarity_scores(row) for row in df_raw.Statement]).join(df_raw)

sentiments.head(5)

import seaborn as sns

ax = sns.boxplot(x='neg', y='Label', data=sentiments , palette='rainbow')

ax = sns.boxplot(x='neu', y='Label', data=sentiments, palette='rainbow')

ax = sns.boxplot(x='pos', y='Label', data=sentiments, palette='rainbow')

"""For each statement, we can extract 4 metrics: negativity, positivity, neutrality and a compound value of all of these metrics.

An interesting observation is that for all 6 available truth labels, all the metrics follow the same distribution, suggesting that the dataset is well balanced in this regard.

-----------------------------------

# Insights

As we see many of the features are important in determining the truthfulness of the statement.

Feature like Venue may also have a contributing factor. A interview may be more misleading than a state speech. 
Or the location may also determine it. A speech given in a remote location may be more misleading than otherwise.

Hence an important aspect is to study the importance of such features and its relationships to build a stronger classifier in future.

Along with that incorporating recent advances in natural language processing like sentiment analysis may also improve in classification results.

# Libraries

* Matplotlib
* Scipy
* Seaborn
* Nltk
* Sklearn
* Pandas

# Code

* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html
* https://stackoverflow.com/questions/29960558/creating-a-bar-plot-using-seaborn
* https://datascienceplus.com/seaborn-categorical-plots-in-python/
* https://stackoverflow.com/questions/12541370/typeerror-encoding-is-an-invalid-keyword-argument-for-this-function/13867190
* https://stackoverflow.com/questions/45890328/sklearn-metrics-for-multiclass-classification
* https://stackoverflow.com/questions/45890328/sklearn-metrics-for-multiclass-classification
* https://scikit-learn.org/stable/modules/multiclass.html
* https://stackoverflow.com/questions/39303912/tfidfvectorizer-in-scikit-learn-valueerror-np-nan-is-an-invalid-document
* https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
"""

